{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1852026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"PROJECTS/dataPoints.xlsx\")\n",
    "data = df.to_numpy()#change to matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to randomly split the data set into a training set, a validation set and a test set. Suggested\n",
    "percentages for this split are 70 %, 15% and 15%, respectively, **but feel free to experiment\n",
    "and change these numbers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeParams(N):\n",
    "    n=2\n",
    "    n_y=1\n",
    "    W = np.random.randn(N,n) \n",
    "    bias = np.random.randn(N,1) \n",
    "    V = np.random.randn(n_y,N) \n",
    "    a=pd.DataFrame(V.T)\n",
    "    a[2]=W[:,0]\n",
    "    a[3]=W[:,1]\n",
    "    a[4]=bias\n",
    "    omega=np.matrix(a)\n",
    "    \n",
    " \n",
    "   \n",
    "    return W,bias,V,omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_rest = train_test_split(data, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test, data_validate = train_test_split(data_rest, test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.transpose(data_train[:,0:2])\n",
    "Y_train = np.transpose(data_train[:,2:])\n",
    "X_validate = np.transpose(data_validate[:,0:2])\n",
    "Y_validate = np.transpose(data_validate[:,2:])\n",
    "X_test = np.transpose(data_test[:,0:2])\n",
    "Y_test = np.transpose(data_test[:,2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = X_train.shape[0] # Size of the input vector X\n",
    "n_y = Y_train.shape[0] # Size of the input vector Y\n",
    "P = X_train.shape[1] # Size of the sample\n",
    "N = 10 # size of the hidden layer, # neurons, this is just for the moment to have some example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.262271</td>\n",
       "      <td>0.746859</td>\n",
       "      <td>0.461002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.804355</td>\n",
       "      <td>0.937081</td>\n",
       "      <td>0.617336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.805674</td>\n",
       "      <td>0.738389</td>\n",
       "      <td>-0.053984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.888805</td>\n",
       "      <td>0.061711</td>\n",
       "      <td>1.943678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.667562</td>\n",
       "      <td>-0.534543</td>\n",
       "      <td>2.127070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2         y\n",
       "0 -1.262271  0.746859  0.461002\n",
       "1 -0.804355  0.937081  0.617336\n",
       "2 -1.805674  0.738389 -0.053984\n",
       "3 -0.888805  0.061711  1.943678\n",
       "4 -1.667562 -0.534543  2.127070"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1. (Full minimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,bias,V,omega=initializeParams(6)\n",
    "ro=10**-5 #10-5 unti, 10-3\n",
    "sigma=1\n",
    "N=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.43849469  0.06912601  0.08928278  0.15155908 -0.73601249 -0.10472084]] \n",
      "\n",
      "[[ 1.08067898  0.68388825]\n",
      " [ 3.2765603   0.03512145]\n",
      " [ 0.08561585 -0.39218973]\n",
      " [-0.59941984  1.1970872 ]\n",
      " [-0.76038139 -0.40552083]\n",
      " [ 1.6745501   1.37888041]] \n",
      "\n",
      "[[ 0.10170382]\n",
      " [ 0.3341281 ]\n",
      " [ 0.11189055]\n",
      " [-0.45791113]\n",
      " [ 1.82330765]\n",
      " [ 0.14409858]]\n"
     ]
    }
   ],
   "source": [
    "print(V,'\\n')\n",
    "print(W,'\\n')\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self, ro, sigma, N):\n",
    "        self.ro = ro\n",
    "        self.sigma = sigma \n",
    "        self.N = N\n",
    "\n",
    "    def set_N(self, x): \n",
    "        self.N = x \n",
    "    def get_N(self): \n",
    "        return self.N     \n",
    "    \n",
    "    \n",
    "    def second_norm(self,omega):\n",
    "        return np.linalg.norm(omega)**2 \n",
    "\n",
    "    def activation_f(self,t,sigma=1):\n",
    "        return (np.exp(2*sigma*t)-1)/(np.exp(2*sigma*t)+1)\n",
    "\n",
    "    def predict(self,omega,X):  \n",
    "\n",
    "        '''\n",
    "        r=omega.flatten().shape[0]\n",
    "        try:\n",
    "            c=omega.flatten().shape[1]\n",
    "\n",
    "            N=int(omega.shape[1]/4)\n",
    "\n",
    "        except:\n",
    "                N=int(omega.shape[0]/4)\n",
    "                \n",
    "        '''     \n",
    "        N=self.N\n",
    "        V=omega.T[:N].reshape(1,N)\n",
    "        W=omega.T[N:N+2*N].reshape(N,2)\n",
    "\n",
    "        bias=omega.T[N+2*N:].reshape(N,1)\n",
    "\n",
    "        t=W.dot(X)-bias\n",
    "\n",
    "\n",
    "        predicted_values=V.dot(self.activation_f(t,sigma))\n",
    "        return predicted_values#, W,bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def reg_tr_error(self,omega,functionArgs):\n",
    "        X=functionArgs[0]\n",
    "        true=functionArgs[1]\n",
    "        \n",
    "        '''\n",
    "        r=omega.flatten().shape[0]\n",
    "        \n",
    "        try:\n",
    "            c=omega.flatten().shape[1]\n",
    "\n",
    "            N=int(omega.shape[1]/4)\n",
    "\n",
    "        except:\n",
    "                N=int(omega.shape[0]/4)\n",
    "\n",
    "        '''\n",
    "        predicted=self.predict(omega,X)\n",
    "\n",
    "\n",
    "        err=np.array(predicted)-true #err_all=np.sum(np.array(predicted)-true)**2\n",
    "        err_all=err.dot(err.T)\n",
    "\n",
    "\n",
    "        P=X.shape[1]\n",
    "        return ((err_all)/(2*P)+self.ro*self.second_norm(omega)).item(0)\n",
    "    \n",
    "\n",
    "  #  def get_params(self):\n",
    "  #      return self.ro, self.sigma, self.N\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, bias, V, omega=initializeParams(7)\n",
    "ro=10**-5 #10-5 unti, 10-3\n",
    "sigma=1\n",
    "N=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4377043473238826"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp=MLP(ro,sigma,7)\n",
    "mlp.reg_tr_error(omega.flatten(),[X_train,Y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.get_N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4377043473238826"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.reg_tr_error(omega.flatten(),[X_train,Y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035109118342736945 \n",
      " [-1.16275922e+01 -3.54894713e+00 -8.67902098e+00  2.24051857e+00\n",
      " -3.75265050e+00  2.81962698e+00 -2.61732684e+00 -1.18092772e-01\n",
      " -9.97126407e-01  1.04547311e+00 -3.37641823e-02 -3.36950197e-02\n",
      " -7.27197097e-01  2.31123757e+00  4.15762143e-02  1.09723692e-02\n",
      "  2.58669930e+00  1.07120950e-01 -1.70941081e+00  2.51340256e-01\n",
      "  3.39135491e+00 -1.06744423e+00 -2.72471876e-01  2.73059955e+00\n",
      "  9.84338921e-01  1.13653889e+00 -1.90642139e+00 -3.80345555e+00]\n"
     ]
    }
   ],
   "source": [
    "res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "print(res['fun'],'\\n',res['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04570328222129813\n",
      "0.02954968028939653\n"
     ]
    }
   ],
   "source": [
    "print(mlp.reg_tr_error(res['x'].flatten(),[X_validate,Y_validate,N]))\n",
    "print(mlp.reg_tr_error(res['x'].flatten(),[X_test,Y_test,N]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E(\\omega,\\pi) = \\frac{1}{2P}\\sum_{p=1}^{P}(f(x^p)-y^p)^2+\\rho \\| \\omega\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "        \n",
    "    the number of neurons N of the hidden layer\n",
    "    the spread delta in the activation function g(t)\n",
    "    the regularization parameter rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAFT VERSION OF THE \"\"HOMEMADE\"\" CROSS-VALIDATION METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the .fit method part is sort of missing cause choosing different configuratuons is needed\n",
    "\n",
    "\n",
    "#repeated CROSS Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_norm_jac(omega): \n",
    "    return np.linalg.norm(omega)\n",
    "\n",
    "def mse(true,predicted):\n",
    "    return (np.sum(np.array(true-predicted)**2))/true.shape[1]\n",
    "\n",
    "\n",
    "def calculate_test_err(cv_data,neurons,rho,sigma,omega):\n",
    "    err=[]\n",
    "    Mlp=MLP(rho,sigma,neurons)\n",
    "    for i in range(10):\n",
    "        X_train = np.transpose(np.matrix(cv_data)[:,0:2])\n",
    "        Y_train = np.transpose(np.matrix(cv_data)[:,2:])\n",
    "        #train on data train and data validate\n",
    "        res=minimize(Mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train,N],method='L-BFGS-B')\n",
    "        omega=res['x']\n",
    "        #test on test data\n",
    "        err.append(mse(Y_test,Mlp.predict(omega.flatten(),X_test)))\n",
    "    return np.mean(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.42808959, -0.11676097, -0.0335933 , -0.21847242,  1.34483608,\n",
       "         -0.59777008,  2.31068604,  0.34143696,  0.81758636,  0.30356997,\n",
       "          0.00572397,  0.81135695]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W,bias,V,omega=initializeParams(N=3)\n",
    "omega.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fivefoldCV(params):\n",
    "    '''\n",
    "    params a list N,rho,sigma\n",
    "    '''\n",
    "    global X_train, X_validate, X_test, Y_train, Y_validate, Y_test \n",
    "\n",
    "    K=5   # SO 4 folds for training 1-validation for testing; they switch every time\n",
    "    cv_data=np.concatenate((data_train,data_validate))\n",
    "    np.random.shuffle(cv_data)\n",
    "    indices=np.arange(0,255,51)# [  0,  51, 102, 153, 204]\n",
    "    folds=[]\n",
    "\n",
    "\n",
    "\n",
    "  #  X=X_train\n",
    "\n",
    "  #  true=Y_train\n",
    "\n",
    "    N=params[0]\n",
    "    rho=params[1]#10**-5 #10-5 unti, 10-3\n",
    "    sigma=params[2]#1\n",
    "\n",
    "    W,bias,V,omega=initializeParams(N)\n",
    "\n",
    "    mlp=MLP(rho,sigma,N)\n",
    "\n",
    "\n",
    "    P=cv_data.shape[0]\n",
    "\n",
    "    mlp.set_N(N)\n",
    "    \n",
    "    #print(mlp.get_N())\n",
    "\n",
    "    val_err_mse=[]\n",
    "    train_err_mse=[]\n",
    "    fun=[]\n",
    "    jac_norm=[]\n",
    "    init_tr_err=[]\n",
    "\n",
    "    #train_err={}\n",
    "    #val_err={}\n",
    "\n",
    "    data=cv_data.copy()\n",
    "\n",
    "\n",
    "    res_df=pd.DataFrame(columns=['neurons','rho','sigma','init_trr_err','fun','err_tr','jac_norm','err_val','err_test'])\n",
    "    for i in range(len(indices)):\n",
    "        cv_data=data\n",
    "        if i<4:\n",
    "            l=[i for i in range(indices[i],indices[i+1])]\n",
    "            #(VALIDATION fold) for testing\n",
    "            validate_cv=cv_data[indices[i]:indices[i+1],:]\n",
    "\n",
    "            #train folds together for training\n",
    "            df=pd.DataFrame(cv_data)\n",
    "            train_cv=df.drop(df.index[l])\n",
    "\n",
    "\n",
    "\n",
    "            X_train = np.transpose(np.matrix(train_cv)[:,0:2])\n",
    "            Y_train = np.transpose(np.matrix(train_cv)[:,2:])\n",
    "            X_validate = np.transpose(validate_cv[:,0:2])\n",
    "            Y_validate = np.transpose(validate_cv[:,2:])\n",
    "            init_tr_err.append( mse(Y_train,mlp.predict(omega.flatten(),X_train)))\n",
    "\n",
    "            #print(omega.shape)\n",
    "        #CHOSEN OMEGA? ->Fitting of the model\n",
    "            res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "     #       for i in range(10):\n",
    "     #           omega=res['x']\n",
    "     #           res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "            \n",
    "            omega=res['x']\n",
    "            fun.append(res['fun'])\n",
    "            jac_norm.append(second_norm_jac(res['jac'].T))\n",
    "\n",
    "        #    err_tr=mse(reg_tr_error(omega.flatten(),[X_train,Y_train]))\n",
    "            err_tr=mse(Y_train,mlp.predict(omega.flatten(),X_train))\n",
    "            err_val=mse(Y_validate,mlp.predict(omega.flatten(),X_validate))\n",
    "\n",
    "            train_err_mse.append(err_tr)\n",
    "            val_err_mse.append(err_val)\n",
    "            \n",
    "             \n",
    "           # train_err_dict[i]=\n",
    "          #  val_err_dict[i]=mse(Y_validate,predict(omega.flatten(),X_validate))\n",
    "\n",
    "\n",
    "        else:\n",
    "            #for the last element\n",
    "            l=list([i for i in range(indices[i],255)])\n",
    "            #(VALIDATION fold) for testing\n",
    "            validate_cv=cv_data[indices[i]:,:]\n",
    "\n",
    "            #train folds together for training\n",
    "            df=pd.DataFrame(cv_data)\n",
    "            train_cv=df.drop(df.index[l])\n",
    "            init_tr_err.append( mse(Y_train,mlp.predict(omega.flatten(),X_train)))\n",
    "            #CHOSEN OMEGA? ->Fitting of the model\n",
    "            res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "          #  for i in range(10):\n",
    "          #      omega=res['x']\n",
    "          #      res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "            \n",
    "            omega=res['x']\n",
    "            fun.append(res['fun'])\n",
    "            jac_norm.append(second_norm_jac(res['jac'].T))\n",
    "\n",
    "\n",
    "    \n",
    "            err_tr=mse(Y_train,mlp.predict(omega.flatten(),X_train))\n",
    "       #     err_tr=mse(reg_tr_error(omega.flatten(),[X_train,Y_train]))\n",
    "            err_val=mse(Y_validate,mlp.predict(omega.flatten(),X_validate))\n",
    "\n",
    "            train_err_mse.append(err_tr)\n",
    "            val_err_mse.append(err_val)\n",
    "\n",
    "      #      train_err_dict_mse[i]=mse(Y_train,predict(omega.flatten(),X_train))\n",
    "       #     val_err_dict_mse[i]=mse(Y_validate,predict(omega,X_validate))\n",
    "\n",
    "    err_test=calculate_test_err(cv_data,N,rho,sigma,omega)\n",
    "    res_df=res_df.append({'neurons':N,'rho':rho,'sigma':sigma,'fun':np.mean(fun),'init_trr_err':np.mean(init_tr_err),\\\n",
    "                              'err_tr':np.mean(train_err_mse),'jac_norm':np.mean(jac_norm),\\\n",
    "                              'err_val':np.mean(val_err_mse),'err_test':err_test},ignore_index=True )\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neurons</th>\n",
       "      <th>rho</th>\n",
       "      <th>sigma</th>\n",
       "      <th>init_trr_err</th>\n",
       "      <th>fun</th>\n",
       "      <th>err_tr</th>\n",
       "      <th>jac_norm</th>\n",
       "      <th>err_val</th>\n",
       "      <th>err_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656607</td>\n",
       "      <td>0.133992</td>\n",
       "      <td>0.248211</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.27378</td>\n",
       "      <td>0.271318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neurons     rho  sigma  init_trr_err       fun    err_tr  jac_norm  \\\n",
       "0      3.0  0.0001    1.0      0.656607  0.133992  0.248211  0.000216   \n",
       "\n",
       "   err_val  err_test  \n",
       "0  0.27378  0.271318  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fivefoldCV([3,0.0001,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_count=[20]#[2,7,11,17,23,27,33]\n",
    "rho_values=[0.001,0.00001]\n",
    "sigma_vals=[0.2,1]\n",
    "\n",
    "all_poss_conf=[]\n",
    "for i in neurons_count:\n",
    "    for r in rho_values:\n",
    "        for s in sigma_vals:\n",
    "            all_poss_conf.append((i,r,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "res_df=pd.DataFrame()\n",
    "for conf in tqdm(all_poss_conf):\n",
    "    res_df=res_df.append(fivefoldCV(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('cv_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho=all_poss_conf[-2][1]\n",
    "sigma=all_poss_conf[-2][2]\n",
    "N=all_poss_conf[-2][0]\n",
    "W,bias,V,omega=initializeParams(N)\n",
    "\n",
    "mlp=MLP(rho,sigma,N)\n",
    "cv_data=np.concatenate((data_train,data_validate))\n",
    "X_train = np.transpose(np.matrix(cv_data)[:,0:2])\n",
    "Y_train = np.transpose(np.matrix(cv_data)[:,2:])\n",
    "res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "omega_star=res['x']\n",
    "\n",
    "\n",
    "Z = mlp.predict(omega_star,X_test)\n",
    "\n",
    "#mlp.predict(omega_star,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mlp.reg_tr_error(res['x'].flatten(),[X_validate,Y_validate,N]))\n",
    "print(mlp.reg_tr_error(omega_star,[X_test,Y_test,N]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((Z-Y_test)**2)/45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN ERROR\n",
    "\n",
    "\n",
    "#TEST ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(): \n",
    " ##   rho=all_poss_conf[-2][1]\n",
    " ##   sigma=all_poss_conf[-2][2]\n",
    " ##   mlp=MLP(rho,sigma)\n",
    "    \n",
    "    fig = plt.figure(figsize=(70,40))\n",
    "    ax = plt.axes(projection='3d')\n",
    "   \n",
    "  #  cv_data=np.concatenate((data_train,data_validate))\n",
    "  #  X_train = np.transpose(np.matrix(cv_data)[:,0:2])\n",
    "  #  Y_train = np.transpose(np.matrix(cv_data)[:,2:])\n",
    "    \n",
    "##    X_train = np.transpose(np.matrix(data)[:,0:2])\n",
    "##    Y_train = np.transpose(np.matrix(data)[:,2:])\n",
    "##\n",
    "##    res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "  #  for i in range(5):\n",
    "  #      omega2=res['x']\n",
    "  #      res=minimize(mlp.reg_tr_error,omega2.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "            \n",
    "    omega2=omega_star\n",
    "    \n",
    " #   xy = np.mgrid[-2:2.002:0.05, -1:1.002:0.05].reshape(2,-1).T\n",
    "    xy = np.mgrid[-1:1.002:0.05, -2:2.002:0.05].reshape(2,-1).T\n",
    "\n",
    "    X = xy[:, 0].squeeze()\n",
    "    Y = xy[:, 1].squeeze()\n",
    "    \n",
    "    XY=np.concatenate((X,Y)).reshape(2,X.shape[0]).T\n",
    "\n",
    "    \n",
    "    Z = mlp.predict(omega_star,XY.T)\n",
    "    Z = Z.T.reshape(X.shape[0],)\n",
    "\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.view_init(azim=30)\n",
    "   # ax.set_zlim(-2,6)\n",
    "    surf = ax.plot_trisurf(X, Y, Z, linewidth=0.010, antialiased=True,cmap='viridis')\n",
    "    fig.savefig('11_MLP')\n",
    "    plt.grid()\n",
    "  #  return X.shape,Y.shape,Z.shape,X,Y,Z.T\n",
    " \n",
    "    \n",
    "   \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(a,v):\n",
    "    return a+v\n",
    "    return a**v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "d['1']=[4]\n",
    "d['1']+=[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1  1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeParams_q12(N):\n",
    "    n=2\n",
    "    n_y=1\n",
    "    idxx = list(np.random.choice(X_train.shape[1], N))\n",
    "    C = X_train[:,idxx].T##np.random.randn(N,n)#X_train[:,idxx].T#\n",
    "    #print(C.shape)\n",
    "    \n",
    "    V = np.random.randn(N,n_y) \n",
    "    a=pd.DataFrame(V)\n",
    "    a[2]=C[:,0]\n",
    "    a[3]=C[:,1]\n",
    "    #a[4]=bias\n",
    "    omega=np.matrix(a)\n",
    "    \n",
    " \n",
    "   \n",
    "    return C,V,omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF(object):\n",
    "    \n",
    "    def __init__(self, ro, sigma, N):\n",
    "        self.ro = ro\n",
    "        self.sigma = sigma \n",
    "        self.N = N\n",
    "\n",
    "    def set_N(self, x): \n",
    "        self.N = x \n",
    "    def get_N(self): \n",
    "        return self.N     \n",
    "    \n",
    "    \n",
    "    def second_norm(self,omega):\n",
    "        return np.linalg.norm(omega)**2 \n",
    "\n",
    "    \n",
    "    def predict0(self,omega,X):  \n",
    "\n",
    "        '''\n",
    "        r=omega.flatten().shape[0]\n",
    "        try:\n",
    "            c=omega.flatten().shape[1]\n",
    "\n",
    "            N=int(omega.shape[1]/4)\n",
    "\n",
    "        except:\n",
    "                N=int(omega.shape[0]/4)\n",
    "                \n",
    "        '''     \n",
    "        N=self.N\n",
    "        omega=omega.flatten().reshape((N,3))\n",
    "        V = omega[:,0]\n",
    "        C = omega[:,1:3]\n",
    "        t1=np.apply_along_axis(np.linalg.norm, 1,X.T - C[0])#X.T - C1\n",
    "        t = np.exp(-(t1/sigma**2)**2)\n",
    "        S=t*V[0].item(0)\n",
    "        for y in range(1,N):\n",
    "            t1=np.apply_along_axis(np.linalg.norm, 1,X.T - C[y])#X.T - C1\n",
    "            t = np.exp(-(t1/sigma**2)**2)\n",
    "            S=S+t*V[y].item(0)\n",
    "        \n",
    "        #predicted_values=S\n",
    "        return S#, W,bias\n",
    "    def predict2(self,omega,X):  \n",
    "\n",
    "        N=self.N\n",
    "        sig = self.sigma\n",
    "        omega=omega.flatten().reshape((N,3))\n",
    "        V = omega[:,0]\n",
    "        #X= X_train\n",
    "        C1 = omega[:,1].T\n",
    "        C2 = omega[:,2].T\n",
    "        X1 = (X[0].T).reshape((X.shape[1],1))\n",
    "        X2 = (X[1].T).reshape((X.shape[1],1))\n",
    "        X1 = X1 - C1\n",
    "        X2 = X2 - C2\n",
    "        XX = -np.square(np.sqrt(np.square(X1)+np.square(X2))/sig)#(np.linalg.norm(X1,axis=1) + np.linalg.norm(X2,axis=1))\n",
    "        XX= np.exp(XX)\n",
    "        CX= XX.dot(V)\n",
    "        CX= CX.reshape((X.shape[1],))\n",
    "        \n",
    "        return CX\n",
    "\n",
    "\n",
    "\n",
    "    def reg_tr_error(self,omega,functionArgs):\n",
    "        X=functionArgs[0]\n",
    "        true=functionArgs[1]\n",
    "        \n",
    "        '''\n",
    "        r=omega.flatten().shape[0]\n",
    "        \n",
    "        try:\n",
    "            c=omega.flatten().shape[1]\n",
    "\n",
    "            N=int(omega.shape[1]/4)\n",
    "\n",
    "        except:\n",
    "                N=int(omega.shape[0]/4)\n",
    "\n",
    "        '''\n",
    "        predicted=self.predict2(omega,X)\n",
    "\n",
    "\n",
    "        err=np.array(predicted)-true #err_all=np.sum(np.array(predicted)-true)**2\n",
    "        err_all=err.dot(err.T)\n",
    "\n",
    "\n",
    "        P=X.shape[1]\n",
    "        return ((err_all)/(2*P)+self.ro*self.second_norm(omega)).item(0)\n",
    "    \n",
    "\n",
    "  #  def get_params(self):\n",
    "  #      return self.ro, self.sigma, self.N\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro=0.0001\n",
    "sigma=1\n",
    "N=2\n",
    "C,V,omega=initializeParams_q12(N)\n",
    "\n",
    "rbf=RBF(ro,sigma,N)\n",
    "cv_data=np.concatenate((data_train,data_validate))\n",
    "X_train = np.transpose(np.matrix(cv_data)[:,0:2])\n",
    "Y_train = np.transpose(np.matrix(cv_data)[:,2:])\n",
    "RR=rbf.predict0(omega.flatten(),X_train)\n",
    "Rr1= rbf.predict2(omega.flatten(),X_train)\n",
    "#print(RR)\n",
    "#print(Rr1)\n",
    "print(Y_train.shape)\n",
    "print(mse(Y_train,Rr1))\n",
    "print(mse(Y_train,RR))\n",
    "\n",
    "st = time.time()\n",
    "res=minimize(rbf.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='CG')\n",
    "omega_star=res['x']\n",
    "print(time.time()-st)\n",
    "print(res['fun'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro=0.00001\n",
    "sigma=1.111\n",
    "N=2\n",
    "rbf=RBF(ro,sigma,N)\n",
    "C,V,omega=initializeParams_q12(N)\n",
    "rbf.predict2(omega.flatten(),X_validate).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = omega[:,0]\n",
    "X= X_train\n",
    "#X=np.expand_dims(X, axis=1)\n",
    "C1 = omega[:,1].T\n",
    "C2 = omega[:,2].T\n",
    "\n",
    "X1 = (X[0].T).reshape((X.shape[1],1))\n",
    "X2 = (X[1].T).reshape((X.shape[1],1))\n",
    "X1 = X1 - C1\n",
    "X2 = X2 - C2\n",
    "XX = -np.square(np.sqrt(np.square(X1)+np.square(X2))/1.2)#(np.linalg.norm(X1,axis=1) + np.linalg.norm(X2,axis=1))\n",
    "XX= np.exp(XX)#(np.linalg.norm(X1,axis=1) + np.linalg.norm(X2,axis=1))\n",
    "CX= XX.dot(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega=omega.flatten().reshape((N,3))\n",
    "V = omega[:,0]\n",
    "C = omega[:,1:3]\n",
    "t1=np.apply_along_axis(np.linalg.norm, 1,X.T - C[0])#X.T - C1\n",
    "t = np.exp(-(t1/1.2)**2)\n",
    "S=t*V[0].item(0)\n",
    "for y in range(1,N):\n",
    "    t1=np.apply_along_axis(np.linalg.norm, 1,X.T - C[y])#X.T - C1\n",
    "    t = np.exp(-(t1/1.2)**2)\n",
    "    S=S+t*V[y].item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(Y_train,CX.flatten())#rbf.predict2(omega_star.flatten(),Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(Y_train,S)#rbf.predict2(omega_star.flatten(),Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fivefoldCV_q12(params):\n",
    "    '''\n",
    "    params a list N,rho,sigma\n",
    "    '''\n",
    "    global X_train, X_validate, X_test, Y_train, Y_validate, Y_test \n",
    "\n",
    "    K=3   # SO 4 folds for training 1-validation for testing; they switch every time\n",
    "    cv_data=np.concatenate((data_train,data_validate))\n",
    "    np.random.shuffle(cv_data)\n",
    "    indices=np.arange(0,255,51)# [  0,  51, 102, 153, 204]\n",
    "    folds=[]\n",
    "\n",
    "\n",
    "\n",
    "  #  X=X_train\n",
    "\n",
    "  #  true=Y_train\n",
    "\n",
    "    N=params[0]\n",
    "    rho=params[1]#10**-5 #10-5 unti, 10-3\n",
    "    sigma=params[2]#1\n",
    "\n",
    "    C,V,omega=initializeParams_q12(N)\n",
    "\n",
    "    rbf=RBF(rho,sigma,N)\n",
    "\n",
    "\n",
    "    P=cv_data.shape[0]\n",
    "\n",
    "    rbf.set_N(N)\n",
    "    \n",
    "    #print(mlp.get_N())\n",
    "\n",
    "    val_err_mse=[]\n",
    "    train_err_mse=[]\n",
    "    fun=[]\n",
    "    jac_norm=[]\n",
    "    init=[]\n",
    "\n",
    "    #train_err={}\n",
    "    #val_err={}\n",
    "\n",
    "    data=cv_data.copy()\n",
    "\n",
    "\n",
    "    res_df=pd.DataFrame(columns=['neurons','rho','sigma','fun','init','err_tr','jac_norm','err_val','err_test'])\n",
    "    for i in range(len(indices)):\n",
    "        cv_data=data\n",
    "        if i<4:\n",
    "            l=[i for i in range(indices[i],indices[i+1])]\n",
    "            #(VALIDATION fold) for testing\n",
    "            validate_cv=cv_data[indices[i]:indices[i+1],:]\n",
    "\n",
    "            #train folds together for training\n",
    "            df=pd.DataFrame(cv_data)\n",
    "            train_cv=df.drop(df.index[l])\n",
    "\n",
    "\n",
    "\n",
    "            X_train = np.transpose(np.matrix(train_cv)[:,0:2])\n",
    "            Y_train = np.transpose(np.matrix(train_cv)[:,2:])\n",
    "            X_validate = np.transpose(validate_cv[:,0:2])\n",
    "            Y_validate = np.transpose(validate_cv[:,2:])\n",
    "            init.append(mse(Y_train,rbf.predict2(omega.flatten(),X_train)))\n",
    "\n",
    "            #print(omega.shape)\n",
    "        #CHOSEN OMEGA? ->Fitting of the model\n",
    "            res=minimize(rbf.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='CG')\n",
    "     #       for i in range(10):\n",
    "     #           omega=res['x']\n",
    "     #           res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "            \n",
    "            omega=res['x']\n",
    "            fun.append(res['fun'])\n",
    "            jac_norm.append(second_norm_jac(res['jac'].T))\n",
    "\n",
    "        #    err_tr=mse(reg_tr_error(omega.flatten(),[X_train,Y_train]))\n",
    "            err_tr=mse(Y_train,rbf.predict2(omega.flatten(),X_train))\n",
    "            err_val=mse(Y_validate,rbf.predict2(omega.flatten(),X_validate))\n",
    "\n",
    "            train_err_mse.append(err_tr)\n",
    "            val_err_mse.append(err_val)\n",
    "            \n",
    "             \n",
    "           # train_err_dict[i]=\n",
    "          #  val_err_dict[i]=mse(Y_validate,predict(omega.flatten(),X_validate))\n",
    "\n",
    "\n",
    "        else:\n",
    "            #for the last element\n",
    "            l=list([i for i in range(indices[i],255)])\n",
    "            #(VALIDATION fold) for testing\n",
    "            validate_cv=cv_data[indices[i]:,:]\n",
    "\n",
    "            #train folds together for training\n",
    "            df=pd.DataFrame(cv_data)\n",
    "            train_cv=df.drop(df.index[l])\n",
    "            init.append(mse(Y_train,rbf.predict2(omega.flatten(),X_train)))\n",
    "\n",
    "            #CHOSEN OMEGA? ->Fitting of the model\n",
    "            res=minimize(rbf.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='CG')\n",
    "          #  for i in range(10):\n",
    "          #      omega=res['x']\n",
    "          #      res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "            \n",
    "            omega=res['x']\n",
    "            fun.append(res['fun'])\n",
    "            jac_norm.append(second_norm_jac(res['jac'].T))\n",
    "\n",
    "\n",
    "    \n",
    "            err_tr=mse(Y_train,rbf.predict2(omega.flatten(),X_train))\n",
    "       #     err_tr=mse(reg_tr_error(omega.flatten(),[X_train,Y_train]))\n",
    "            err_val=mse(Y_validate,rbf.predict2(omega.flatten(),X_validate))\n",
    "\n",
    "            train_err_mse.append(err_tr)\n",
    "            val_err_mse.append(err_val)\n",
    "\n",
    "      #      train_err_dict_mse[i]=mse(Y_train,predict(omega.flatten(),X_train))\n",
    "       #     val_err_dict_mse[i]=mse(Y_validate,predict(omega,X_validate))\n",
    "\n",
    "    err_test=calculate_test_err(cv_data,N,rho,sigma,omega)\n",
    "    res_df=res_df.append({'neurons':N,'rho':rho,'sigma':sigma,'fun':np.mean(fun),'init':np.mean(init),\\\n",
    "                              'err_tr':np.mean(train_err_mse),'jac_norm':np.mean(jac_norm),\\\n",
    "                              'err_val':np.mean(val_err_mse),'err_test':err_test},ignore_index=True )\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_norm_jac(omega): \n",
    "    return np.linalg.norm(omega)\n",
    "\n",
    "def mse(true,predicted):\n",
    "    return (np.sum(np.array(true-predicted)**2))/true.shape[1]\n",
    "\n",
    "\n",
    "def calculate_test_err(cv_data,neurons,rho,sigma,omega):\n",
    "    err=[]\n",
    "    Rbf=RBF(rho,sigma,neurons)\n",
    "    for i in range(10):\n",
    "        X_train = np.transpose(np.matrix(cv_data)[:,0:2])\n",
    "        Y_train = np.transpose(np.matrix(cv_data)[:,2:])\n",
    "        #train on data train and data validate\n",
    "        res=minimize(Rbf.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='CG')\n",
    "        omega=res['x']\n",
    "        #test on test data\n",
    "        err.append(mse(Y_test,Rbf.predict2(omega.flatten(),X_test)))\n",
    "    return np.mean(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_count=[2,9,17,25,33]\n",
    "rho_values=[0.001,0.0001,0.00001]\n",
    "sigma_vals=[1.11,1.455,1.99]\n",
    "\n",
    "all_poss_conf=[]\n",
    "for i in neurons_count:\n",
    "    for r in rho_values:\n",
    "        for s in sigma_vals:\n",
    "            all_poss_conf.append((i,r,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_data():\n",
    "    data = df.to_numpy()\n",
    "    data_train, data_rest = train_test_split(data, test_size=0.30)\n",
    "    data_test, data_val = train_test_split(data_rest, test_size=0.50)\n",
    "    data_train, data_validate, data_test = np.split(df.sample(frac=1), [int(.7*len(df)), int(.85*len(df))])\n",
    "    data_train = data_train.to_numpy()\n",
    "    data_validate = data_validate.to_numpy()\n",
    "    data_test = data_test.to_numpy()\n",
    "\n",
    "    X_tr = np.transpose(data_train[:,0:2])\n",
    "    Y_tr = np.transpose(data_train[:,2:])\n",
    "    X_val = np.transpose(data_validate[:,0:2])\n",
    "    Y_val = np.transpose(data_validate[:,2:])\n",
    "    X_test = np.transpose(data_test[:,0:2])\n",
    "    Y_test = np.transpose(data_test[:,2:])\n",
    "    return X_tr,Y_tr,X_val,Y_val,X_test,Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X, y_true, Xval,Yval, Xtest, Ytest = get_data()\n",
    "funn,starting_err,ending_err,jac_norm,val_err,test_err=[],[],[],[],[],[]\n",
    "conf_q2=pd.DataFrame(columns=['neurons','rho','sigma','fun','start_err'\\\n",
    "                                            ,'ending_err','jac_norm',\\\n",
    "                                           'val_err','test_err'])\n",
    "for conf in tqdm(all_poss_conf):\n",
    "    C,V,omega=initializeParams_q12(conf[0])\n",
    "\n",
    "    rbf=RBF(conf[1],conf[2],conf[0])\n",
    "    res=minimize(rbf.reg_tr_error,omega.flatten(), args=[X,y_true],method='L-BFGS-B')\n",
    "    omega_star=res['x']\n",
    "    funn=res['fun']\n",
    "    #pred_i = rbf.predict(omega.flatten(),X)\n",
    "    starting_err=mse(y_true,rbf.predict2(omega.flatten(),X))\n",
    "    ending_err=mse(y_true,rbf.predict2(omega_star.flatten(),X))\n",
    "    jac_norm=second_norm_jac(res['jac'].T)\n",
    "    val_err=mse(Yval,rbf.predict(omega_star.flatten(),Xval))\n",
    "    X=np.concatenate((X.T,Xval.T)).T\n",
    "    y_true=np.concatenate((y_true.T,Yval.T)).T\n",
    "    res=minimize(rbf.reg_tr_error,omega_star.flatten(), args=[X,y_true],method='L-BFGS-B')\n",
    "    omega_star=res['x']\n",
    "    test_err=mse(Ytest,rbf.predict2(omega_star.flatten(),Xtest))\n",
    "    conf_q2=conf_q2.append({'neurons':conf[0],'rho':conf[1],'sigma':conf[2],'fun':funn,'start_err':starting_err,\\\n",
    "                              'ending_err':funn,'jac_norm':jac_norm,\\\n",
    "                              'val_err':val_err,'test_err':test_err},ignore_index=True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conf_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fivefoldCV_q12([3,0.0001,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "res_df=pd.DataFrame()\n",
    "for conf in tqdm(all_poss_conf):\n",
    "    res_df=res_df.append(fivefoldCV_q12(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('q12_trial1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_count=[50,88,125]\n",
    "rho_values=[0.001,0.0001,0.00001]\n",
    "sigma_vals=[1.11,1.455,1.99]\n",
    "\n",
    "all_poss_conf=[]\n",
    "for i in neurons_count:\n",
    "    for r in rho_values:\n",
    "        for s in sigma_vals:\n",
    "            all_poss_conf.append((i,r,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "res_df=pd.DataFrame()\n",
    "for conf in tqdm(all_poss_conf):\n",
    "    res_df=res_df.append(fivefoldCV_q12(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('q12_trial1_bigvals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Extreme Learning 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Q2(object):\n",
    "    \n",
    "    def __init__(self, ro, sigma, N):\n",
    "        self.ro = ro\n",
    "        self.sigma = sigma \n",
    "        self.N = N\n",
    "\n",
    "    def set_N(self, x): \n",
    "        self.N = x \n",
    "    def get_N(self): \n",
    "        return self.N     \n",
    "    \n",
    "    \n",
    "    def second_norm(self,omega):\n",
    "        return np.linalg.norm(omega)**2 \n",
    "\n",
    "    def activation_f(self,t,sigma=1):\n",
    "        return (np.exp(2*sigma*t)-1)/(np.exp(2*sigma*t)+1)\n",
    "\n",
    "    def predict(self,X, W, bias, V):  \n",
    "  \n",
    "        N=self.N\n",
    "        t=W.dot(X)-bias\n",
    "        #print(t.shape)\n",
    "        predicted_values=V.dot(self.activation_f(t,sigma))\n",
    "        return predicted_values#, W,bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def reg_tr_error(self,V,functionArgs):\n",
    "        X=functionArgs[0]\n",
    "        true=functionArgs[1]\n",
    "        W = functionArgs[2]\n",
    "        bias = functionArgs[3]\n",
    "        \n",
    "        predicted=self.predict(X,W,bias,V)\n",
    "\n",
    "\n",
    "        err=np.array(predicted)-true #err_all=np.sum(np.array(predicted)-true)**2\n",
    "        err_all=err.dot(err.T)\n",
    "\n",
    "\n",
    "        P=X.shape[1]\n",
    "        #omega_upd=np.append(np.append(V,W, axis=1), bias, axis=1)\n",
    "        a=pd.DataFrame(V.T)\n",
    "        a[2]=W[:,0]\n",
    "        a[3]=W[:,1]\n",
    "        a[4]=bias\n",
    "        omega_upd=np.matrix(a)\n",
    "        return ((err_all)/(2*P)+self.ro*self.second_norm(omega_upd)).item(0)\n",
    "    \n",
    "\n",
    "  #  def get_params(self):\n",
    "  #      return self.ro, self.sigma, self.N\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeParams_q2(N):\n",
    "    n=2\n",
    "    n_y=1\n",
    "    W = np.random.randn(N,n)*np.random.randn(N,n)*np.random.randn(N,n)\n",
    "    bias = np.random.randn(N,1)*np.random.randn(N,1)*np.random.randn(N,1)\n",
    "    V = np.random.randn(n_y,N) \n",
    "    a=pd.DataFrame(V.T)\n",
    "    a[2]=W[:,0]\n",
    "    a[3]=W[:,1]\n",
    "    a[4]=bias\n",
    "    omega=np.matrix(a)\n",
    "    \n",
    " \n",
    "   \n",
    "    return W,bias,V,omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "ro=0.00001#all_poss_conf[-2][1]\n",
    "sigma=1#all_poss_conf[-2][2]\n",
    "N=13#all_poss_conf[-2][0]\n",
    "W,bias,V,omega=initializeParams_q2(N)\n",
    "\n",
    "mlp_q2=MLP_Q2(ro,sigma,N)\n",
    "cv_data=np.concatenate((data_train,data_validate))\n",
    "X_train = np.transpose(np.matrix(cv_data)[:,0:2])\n",
    "Y_train = np.transpose(np.matrix(cv_data)[:,2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=minimize(mlp_q2.reg_tr_error,V, args=[X_train,Y_train, W, bias],method='CG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_star=res['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp_q2.reg_tr_error(V_star,[X_train,Y_train,W,bias]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp_q2.reg_tr_error(V_star,[X_test,Y_test,W,bias]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_2_mlp(rho,sigma,N,rep, X,Y,X_test,Y_test):\n",
    "    test_err=[]\n",
    "    train_err=[]\n",
    "    from tqdm import tqdm\n",
    "    for r in tqdm(range(rep)):\n",
    "        W,bias,V,omega=initializeParams_q2(N)\n",
    "        mlp=MLP_Q2(rho,sigma,N)\n",
    "        res=minimize(mlp.reg_tr_error,V, args=[X,Y, W, bias],method='CG')\n",
    "        V_star=res['x']\n",
    "        curr_err=mlp.reg_tr_error(V_star,[X_test,Y_test,W,bias])\n",
    "        test_err.append(curr_err)\n",
    "        train_err.append(mlp.reg_tr_error(V_star,[X,Y,W,bias]))\n",
    "        if np.min(test_err)==curr_err:\n",
    "            best = (W,bias,V_star)\n",
    "    return best,test_err,train_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_V_W_b,test_errs,train_errs=q_2_mlp(ro,sigma,N,10, X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WW=best_V_W_b[0]\n",
    "res_q2=pd.DataFrame(WW[:,0])\n",
    "res_q2[2]=WW[:,1]\n",
    "res_q2[3]=best_V_W_b[1]\n",
    "res_q2[4]=best_V_W_b[2]\n",
    "res_q2.to_csv('res_q2_W_b_V.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_q2=pd.read_csv('res_q2_W_b_V.csv',usecols=[1,2,3,4])\n",
    "omega_upd=res_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_upd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_V_W_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_q2=MLP_Q2(ro,sigma,N)\n",
    "print(mlp_q2.reg_tr_error(best_V_W_b[2],[X_test,Y_test,best_V_W_b[0],best_V_W_b[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_q2(W_inp,bias_inp,V_inp): \n",
    " ##   rho=all_poss_conf[-2][1]\n",
    " ##   sigma=all_poss_conf[-2][2]\n",
    " ##   mlp=MLP(rho,sigma)\n",
    "    \n",
    "    fig = plt.figure(figsize=(70,40))\n",
    "    ax = plt.axes(projection='3d')\n",
    "   \n",
    "  #  cv_data=np.concatenate((data_train,data_validate))\n",
    "  #  X_train = np.transpose(np.matrix(cv_data)[:,0:2])\n",
    "  #  Y_train = np.transpose(np.matrix(cv_data)[:,2:])\n",
    "    \n",
    "##    X_train = np.transpose(np.matrix(data)[:,0:2])\n",
    "##    Y_train = np.transpose(np.matrix(data)[:,2:])\n",
    "##\n",
    "##    res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "  #  for i in range(5):\n",
    "  #      omega2=res['x']\n",
    "  #      res=minimize(mlp.reg_tr_error,omega2.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "            \n",
    "    #omega2=omega_star\n",
    "    a=pd.DataFrame(V_inp.T)\n",
    "    a[2]=W_inp[:,0]\n",
    "    a[3]=W_inp[:,1]\n",
    "    a[4]=bias_inp\n",
    "    omega2=np.matrix(a)\n",
    "    \n",
    " #   xy = np.mgrid[-2:2.002:0.05, -1:1.002:0.05].reshape(2,-1).T\n",
    "    xy = np.mgrid[-1:1.002:0.05, -2:2.002:0.05].reshape(2,-1).T\n",
    "\n",
    "    X = xy[:, 0].squeeze()\n",
    "    Y = xy[:, 1].squeeze()\n",
    "    \n",
    "    XY=np.concatenate((X,Y)).reshape(2,X.shape[0]).T\n",
    "    #print(XY)\n",
    "\n",
    "    \n",
    "    Z = mlp_q2.predict(XY.T,W_inp,bias_inp,V_inp)\n",
    "    Z = Z.T.reshape(X.shape[0],)\n",
    "\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.view_init(azim=30)\n",
    "   # ax.set_zlim(-2,6)\n",
    "    surf = ax.plot_trisurf(X, Y, Z, linewidth=0.010, antialiased=True,cmap='viridis')\n",
    "    fig.savefig('11_MLP')\n",
    "    plt.grid()\n",
    "  #  return X.shape,Y.shape,Z.shape,X,Y,Z.T\n",
    " \n",
    "    \n",
    "   \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu=omega_upd.values[:,0:2]\n",
    "bb=omega_upd.values[:,2].reshape(N,1)\n",
    "vv=omega_upd.values[:,3].reshape(N,1).T\n",
    "mlp_q2=MLP_Q2(ro,sigma,N)\n",
    "print(mlp_q2.reg_tr_error(vv,[X_test,Y_test,np.array(uu),bb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_q2(np.array(uu),bb,vv)# with 13 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_q2(np.array(uu),bb,vv)# with 25 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_q2(np.array(uu),bb,vv)# with 20 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_q2(np.array(uu),bb,vv)# with 33 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_q2(np.array(uu),bb,vv)# with 23 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_q2(best_V_W_b[0],best_V_W_b[1],best_V_W_b[2]) # with 17 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_q2(best_V_W_b[0],best_V_W_b[1],best_V_W_b[2]) # with 3 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Unsupervised selection of centers 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF_Q2(object):\n",
    "    \n",
    "    def __init__(self, ro, sigma, N):\n",
    "        self.ro = ro\n",
    "        self.sigma = sigma \n",
    "        self.N = N\n",
    "\n",
    "    def set_N(self, x): \n",
    "        self.N = x \n",
    "    def get_N(self): \n",
    "        return self.N     \n",
    "    \n",
    "    \n",
    "    def second_norm(self,omega):\n",
    "        return np.linalg.norm(omega)**2 \n",
    "\n",
    "    \n",
    "    def predict0(self,omega,X):  \n",
    "\n",
    "        '''\n",
    "        r=omega.flatten().shape[0]\n",
    "        try:\n",
    "            c=omega.flatten().shape[1]\n",
    "\n",
    "            N=int(omega.shape[1]/4)\n",
    "\n",
    "        except:\n",
    "                N=int(omega.shape[0]/4)\n",
    "                \n",
    "        '''     \n",
    "        N=self.N\n",
    "        omega=omega.flatten().reshape((N,3))\n",
    "        V = omega[:,0]\n",
    "        C = omega[:,1:3]\n",
    "        t1=np.apply_along_axis(np.linalg.norm, 1,X.T - C[0])#X.T - C1\n",
    "        t = np.exp(-(t1/sigma**2)**2)\n",
    "        S=t*V[0].item(0)\n",
    "        for y in range(1,N):\n",
    "            t1=np.apply_along_axis(np.linalg.norm, 1,X.T - C[y])#X.T - C1\n",
    "            t = np.exp(-(t1/sigma**2)**2)\n",
    "            S=S+t*V[y].item(0)\n",
    "        \n",
    "        #predicted_values=S\n",
    "        return S#, W,bias\n",
    "    def predict2(self,omega,X):  \n",
    "\n",
    "        N=self.N\n",
    "        sig = self.sigma\n",
    "        omega=omega.flatten().reshape((N,3))\n",
    "        V = omega[:,0]\n",
    "        #X= X_train\n",
    "        C1 = omega[:,1].T\n",
    "        C2 = omega[:,2].T\n",
    "        X1 = (X[0].T).reshape((X.shape[1],1))\n",
    "        X2 = (X[1].T).reshape((X.shape[1],1))\n",
    "        X1 = X1 - C1\n",
    "        X2 = X2 - C2\n",
    "        XX = -np.square(np.sqrt(np.square(X1)+np.square(X2))/sig)#(np.linalg.norm(X1,axis=1) + np.linalg.norm(X2,axis=1))\n",
    "        XX= np.exp(XX)\n",
    "        CX= XX.dot(V)\n",
    "        CX= CX.reshape((X.shape[1],))\n",
    "        \n",
    "        return CX\n",
    "\n",
    "    def predict22(self,V,C,X):  \n",
    "\n",
    "        N=self.N\n",
    "        sig = self.sigma\n",
    "        V=V.flatten().reshape((N,1))\n",
    "        #X= X_train\n",
    "        C1 = C[:,0].T\n",
    "        C2 = C[:,1].T\n",
    "        X1 = (X[0].T).reshape((X.shape[1],1))\n",
    "        X2 = (X[1].T).reshape((X.shape[1],1))\n",
    "        X1 = X1 - C1\n",
    "        X2 = X2 - C2\n",
    "        XX = -np.square(np.sqrt(np.square(X1)+np.square(X2))/sig)#(np.linalg.norm(X1,axis=1) + np.linalg.norm(X2,axis=1))\n",
    "        XX= np.exp(XX)\n",
    "        CX= XX.dot(V)\n",
    "        CX= CX.reshape((X.shape[1],))\n",
    "        \n",
    "        return CX\n",
    "\n",
    "\n",
    "    def reg_tr_error(self,V,functionArgs):\n",
    "        X=functionArgs[0]\n",
    "        true=functionArgs[1]\n",
    "        C=functionArgs[2]\n",
    "\n",
    "        V = V.flatten().reshape((self.N,1))\n",
    "        \n",
    "        '''\n",
    "        r=omega.flatten().shape[0]\n",
    "        \n",
    "        try:\n",
    "            c=omega.flatten().shape[1]\n",
    "\n",
    "            N=int(omega.shape[1]/4)\n",
    "\n",
    "        except:\n",
    "                N=int(omega.shape[0]/4)\n",
    "\n",
    "        '''\n",
    "        predicted=self.predict22(V,C,X)\n",
    "\n",
    "\n",
    "        err=np.array(predicted)-true #err_all=np.sum(np.array(predicted)-true)**2\n",
    "        err_all=err.dot(err.T)\n",
    "\n",
    "\n",
    "        P=X.shape[1]\n",
    "        return ((err_all)/(2*P)+self.ro*self.second_norm(omega)).item(0)\n",
    "    \n",
    "\n",
    "  #  def get_params(self):\n",
    "  #      return self.ro, self.sigma, self.N\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini = 10 #number of repetitions to reduce the beginnings with \"bad points\"\n",
    "N = 50\n",
    "rho = 0.00001\n",
    "sigma = 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_centers(X,N,ini=2):\n",
    "    for _ in range(ini):\n",
    "    # define the random initial parameters\n",
    "        v_init = [random.uniform(0, 1) for i in range(N)]\n",
    "        \n",
    "        kmeans=KMeans(n_clusters=N, random_state=_)\n",
    "        a=kmeans.fit(X.T)\n",
    "        c_init=a.cluster_centers_\n",
    "    return c_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeParams_q22(N,X):\n",
    "    n_y=1\n",
    "    V = np.random.randn(N,n_y) # Maybe we should choose some small values (0,1)????\n",
    "    C = choose_centers(X,N,10)\n",
    "    \n",
    "    a=pd.DataFrame(V)\n",
    "    a[2]=C[:,0]\n",
    "    a[3]=C[:,1]\n",
    "    omega=np.matrix(a)\n",
    "    \n",
    "    return V,C,omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "V,C,omega = initializeParams_q22(N,X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of the regularized error:\n",
    "$$\\nabla_{v} E(v,c)=\\frac{1}{P}\\Phi ^{T}(\\Phi v \\;- \\;Y)\\;+\\;2 \\rho v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def grad_reg_error(V, args):\n",
    "    \n",
    "    X = args[0]\n",
    "    Y = args[1]\n",
    "    C0 = args[2]\n",
    "    N = args[3]\n",
    "    rho = args[4]\n",
    "    sigma = args[5]\n",
    "    \n",
    "    \n",
    "    V=V.flatten().reshape((N,1))\n",
    "    \n",
    "    #V = omega[:,0]\n",
    "    #C1 = omega[:,1].T\n",
    "    #C2 = omega[:,2].T\n",
    "    C1 = C0[:,0].T\n",
    "    C2 = C0[:,1].T\n",
    "    X1 = (X[0].T).reshape((X.shape[1],1))\n",
    "    X2 = (X[1].T).reshape((X.shape[1],1))\n",
    "    X1 = X1 - C1\n",
    "    X2 = X2 - C2\n",
    "    XX = -np.square(np.sqrt(np.square(X1)+np.square(X2))/sigma)\n",
    "    Phi= np.exp(XX)\n",
    "    grad = (1/X.shape[1])*((Phi.T).dot((Phi.dot(V))-Y.T))+2*rho*V\n",
    "    \n",
    "    return grad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_norm_jac(omega): \n",
    "    return np.linalg.norm(omega)\n",
    "\n",
    "def mse(true,predicted):\n",
    "    return (np.sum(np.array(true-predicted)**2))/true.shape[1]\n",
    "\n",
    "\n",
    "def calculate_test_err(cv_data,C,neurons,rho,sigma,V):\n",
    "    err=[]\n",
    "    Rbf=RBF_Q2(rho,sigma,neurons)\n",
    "    for i in range(10):\n",
    "        X_train = np.transpose(np.matrix(cv_data)[:,0:2])\n",
    "        Y_train = np.transpose(np.matrix(cv_data)[:,2:])\n",
    "        #train on data train and data validate\n",
    "        res=minimize(Rbf.reg_tr_error,V.flatten(), args=[X_train,Y_train,C,neurons,rho,sigma],method='CG',jac=grad_reg_error)\n",
    "        V=res['x']\n",
    "        #test on test data\n",
    "        err.append(mse(Y_test,Rbf.predict22(V.flatten(),C,X_test)))\n",
    "    return np.mean(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fivefoldCV_q22(params):\n",
    "    '''\n",
    "    params a list N,rho,sigma\n",
    "    '''\n",
    "    global X_train, X_validate, X_test, Y_train, Y_validate, Y_test \n",
    "\n",
    "    K=5   # SO 4 folds for training 1-validation for testing; they switch every time\n",
    "    cv_data=np.concatenate((data_train,data_validate))\n",
    "    np.random.shuffle(cv_data)\n",
    "    indices=np.arange(0,255,51)# [  0,  51, 102, 153, 204]\n",
    "    folds=[]\n",
    "\n",
    "\n",
    "\n",
    "  #  X=X_train\n",
    "\n",
    "  #  true=Y_train\n",
    "\n",
    "    N=params[0]\n",
    "    rho=params[1]#10**-5 #10-5 unti, 10-3\n",
    "    sigma=params[2]#1\n",
    "\n",
    "    V,C0,omega = initializeParams_q22(N,X_train)\n",
    "\n",
    "    rbf=RBF_Q2(rho,sigma,N)\n",
    "\n",
    "\n",
    "    P=cv_data.shape[0]\n",
    "\n",
    "    rbf.set_N(N)\n",
    "    \n",
    "    #print(mlp.get_N())\n",
    "\n",
    "    val_err_mse=[]\n",
    "    train_err_mse=[]\n",
    "    fun=[]\n",
    "    jac_norm=[]\n",
    "    init=[]\n",
    "\n",
    "    #train_err={}\n",
    "    #val_err={}\n",
    "\n",
    "    data=cv_data.copy()\n",
    "\n",
    "\n",
    "    res_df=pd.DataFrame(columns=['neurons','rho','sigma','fun','init','err_tr','jac_norm','err_val'])\n",
    "    for i in range(len(indices)):\n",
    "        cv_data=data\n",
    "        if i<4:\n",
    "            l=[i for i in range(indices[i],indices[i+1])]\n",
    "            #(VALIDATION fold) for testing\n",
    "            validate_cv=cv_data[indices[i]:indices[i+1],:]\n",
    "\n",
    "            #train folds together for training\n",
    "            df=pd.DataFrame(cv_data)\n",
    "            train_cv=df.drop(df.index[l])\n",
    "\n",
    "\n",
    "\n",
    "            X_train = np.transpose(np.matrix(train_cv)[:,0:2])\n",
    "            Y_train = np.transpose(np.matrix(train_cv)[:,2:])\n",
    "            X_validate = np.transpose(validate_cv[:,0:2])\n",
    "            Y_validate = np.transpose(validate_cv[:,2:])\n",
    "            init.append(mse(Y_train,rbf.predict22(V,C,X_train)))\n",
    "\n",
    "            #print(omega.shape)\n",
    "        #CHOSEN OMEGA? ->Fitting of the model\n",
    "            res=minimize(rbf.reg_tr_error,V, args=[X_train,Y_train,C,N,rho,sigma],method='CG',jac=grad_reg_error)\n",
    "     #       for i in range(10):\n",
    "     #           omega=res['x']\n",
    "     #           res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "            \n",
    "            V=res['x']\n",
    "            fun.append(res['fun'])\n",
    "            jac_norm.append(second_norm_jac(res['jac'].T))\n",
    "\n",
    "        #    err_tr=mse(reg_tr_error(omega.flatten(),[X_train,Y_train]))\n",
    "            err_tr=mse(Y_train,rbf.predict22(V,C,X_train))\n",
    "            err_val=mse(Y_validate,rbf.predict22(V,C,X_validate))\n",
    "\n",
    "            train_err_mse.append(err_tr)\n",
    "            val_err_mse.append(err_val)\n",
    "            \n",
    "             \n",
    "           # train_err_dict[i]=\n",
    "          #  val_err_dict[i]=mse(Y_validate,predict(omega.flatten(),X_validate))\n",
    "\n",
    "\n",
    "        else:\n",
    "            #for the last element\n",
    "            l=list([i for i in range(indices[i],255)])\n",
    "            #(VALIDATION fold) for testing\n",
    "            validate_cv=cv_data[indices[i]:,:]\n",
    "\n",
    "            #train folds together for training\n",
    "            df=pd.DataFrame(cv_data)\n",
    "            train_cv=df.drop(df.index[l])\n",
    "            init.append(mse(Y_train,rbf.predict22(V,C,X_train)))\n",
    "\n",
    "            #CHOSEN OMEGA? ->Fitting of the model\n",
    "            res=minimize(rbf.reg_tr_error,V, args=[X_train,Y_train,C,N,rho,sigma],method='CG',jac=grad_reg_error)\n",
    "          #  for i in range(10):\n",
    "          #      omega=res['x']\n",
    "          #      res=minimize(mlp.reg_tr_error,omega.flatten(), args=[X_train,Y_train],method='L-BFGS-B')\n",
    "            \n",
    "            V=res['x']\n",
    "            fun.append(res['fun'])\n",
    "            jac_norm.append(second_norm_jac(res['jac'].T))\n",
    "\n",
    "\n",
    "    \n",
    "            err_tr=mse(Y_train,rbf.predict22(V,C,X_train))\n",
    "       #     err_tr=mse(reg_tr_error(omega.flatten(),[X_train,Y_train]))\n",
    "            err_val=mse(Y_validate,rbf.predict22(V,C,X_validate))\n",
    "\n",
    "            train_err_mse.append(err_tr)\n",
    "            val_err_mse.append(err_val)\n",
    "\n",
    "      #      train_err_dict_mse[i]=mse(Y_train,predict(omega.flatten(),X_train))\n",
    "       #     val_err_dict_mse[i]=mse(Y_validate,predict(omega,X_validate))\n",
    "\n",
    "   # err_test=calculate_test_err(cv_data,N,rho,sigma,omega,V)\n",
    "    res_df=res_df.append({'neurons':N,'rho':rho,'sigma':sigma,'fun':np.mean(fun),'init':np.mean(init),\\\n",
    "                              'err_tr':np.mean(train_err_mse),'jac_norm':np.mean(jac_norm),\\\n",
    "                              'err_val':np.mean(val_err_mse)},ignore_index=True )\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to do optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 1e-05, 1.11]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars =[N,rho,sigma]\n",
    "V,C,omega = initializeParams_q22(N,X_train)\n",
    "args=[X_train,Y_train,C,N,rho,sigma]\n",
    "rbf = RBF_Q2(rho,sigma,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = args[0]\n",
    "Y = args[1]\n",
    "C0 = args[2]\n",
    "N = args[3]\n",
    "rho = args[4]\n",
    "sigma = args[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=rbf.reg_tr_error(V.flatten(),args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.32063238690182"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a # starting value of the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_reg_error(V, args):\n",
    "    try:\n",
    "        X = args[0]\n",
    "        Y = args[1]\n",
    "        C0 = args[2]\n",
    "        N = args[3]\n",
    "        rho = args[4]\n",
    "        sigma = args[5]\n",
    "    \n",
    "    \n",
    "        V=V.flatten().reshape((N,1))\n",
    "\n",
    "        #V = omega[:,0]\n",
    "        #C1 = omega[:,1].T\n",
    "        #C2 = omega[:,2].T\n",
    "        C1 = C0[:,0].T\n",
    "        C2 = C0[:,1].T\n",
    "   \n",
    "        X1 = (X[0].T).reshape((X.shape[1],1))\n",
    "        X2 = (X[1].T).reshape((X.shape[1],1))\n",
    "  \n",
    "        X1 = X1 - C1\n",
    "        X2 = X2 - C2\n",
    "#        print(4)\n",
    "\n",
    "        XX = -np.square(np.sqrt(np.square(X1)+np.square(X2))/sigma)\n",
    "#        print(5)\n",
    "\n",
    "\n",
    "        Phi= np.exp(XX)\n",
    "#        print('phi',Phi.shape)\n",
    "#        print('rho',rho)\n",
    "#        print('V',V.shape)\n",
    "#        print('dssa',((Phi.dot(V))-Y.T).shape) #204x1\n",
    "#        print('xx',(((Phi.T).dot((Phi.dot(V))-Y.T))).shape)\n",
    "#        print('da',V.shape[0])\n",
    "#        print((1/X.shape[1])*(((Phi.T).dot((Phi.dot(V))-Y.T))).flatten())#.reshape((X.shape[1],1))\n",
    "        \n",
    "##        print((2*rho*V).shape)\n",
    "        #        grad = (1/X.shape[1])*((Phi.T).dot((Phi.dot(V))-Y.T))+2*rho*V\n",
    "        grad1=((1/X.shape[1])*(((Phi.T).dot((Phi.dot(V))-Y.T))))#.flatten()\n",
    "  ##      print('ds',grad1.shape)\n",
    "        gg=(2*rho*V)#.reshape((X.shape[1],1))\n",
    "##        print('1',grad1.flatten()[0],'shape',grad1.shape)\n",
    "##        print('2',gg.shape)\n",
    "        grad=np.sum([grad1,gg],axis=0)#.flatten()\n",
    "   #     grad = (1/X.shape[1])*((Phi.T).dot((Phi.dot(V))-Y.T))+2*rho*V\n",
    "    except:\n",
    "        print('mjau')  \n",
    "    return grad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_reg_error(V, args):\n",
    "    try:\n",
    "        X = args[0]\n",
    "        Y = args[1]\n",
    "        C0 = args[2]\n",
    "        N = args[3]\n",
    "        rho = args[4]\n",
    "        sigma = args[5]\n",
    "    \n",
    "    \n",
    "        V=V.flatten().reshape((N,1))\n",
    "\n",
    "        #V = omega[:,0]\n",
    "        #C1 = omega[:,1].T\n",
    "        #C2 = omega[:,2].T\n",
    "        C1 = C0[:,0].T\n",
    "        C2 = C0[:,1].T\n",
    "   \n",
    "        X1 = (X[0].T).reshape((X.shape[1],1))\n",
    "        X2 = (X[1].T).reshape((X.shape[1],1))\n",
    "  \n",
    "        X1 = X1 - C1\n",
    "        X2 = X2 - C2\n",
    "\n",
    "        XX = -np.square(np.sqrt(np.square(X1)+np.square(X2))/sigma)\n",
    "\n",
    "        \n",
    "\n",
    "        Phi= np.exp(XX)\n",
    "#     \n",
    "\n",
    "        #        grad = (1/X.shape[1])*((Phi.T).dot((Phi.dot(V))-Y.T))+2*rho*V\n",
    "        grad1=((1/X.shape[1])*(((Phi.T).dot((Phi.dot(V))-Y.T))))#.flatten()\n",
    "  ##      print('ds',grad1.shape)\n",
    "        gg=(2*rho*V)#.reshape((X.shape[1],1))\n",
    "##        print('1',grad1.flatten()[0],'shape',grad1.shape)\n",
    "##        print('2',gg.shape)\n",
    "        grad=np.sum([grad1,gg],axis=0)#.flatten()\n",
    "   #     grad = (1/X.shape[1])*((Phi.T).dot((Phi.dot(V))-Y.T))+2*rho*V\n",
    "    except:\n",
    "        print('mjau')\n",
    "    return grad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_reg_error(V.flatten(), args=[X_train,Y_train,C,N,rho,sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=minimize(rbf.reg_tr_error,V.flatten(), args=[X_train,Y_train,C,N,rho,sigma],method='CG',jac=grad_reg_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.053020115066551125\n",
       "     jac: array([-1.12041289e-06, -7.44488737e-06, -3.69582877e-06, -3.95945790e-06,\n",
       "       -5.85806771e-06, -5.04973567e-06, -6.01939641e-06, -5.53130864e-06,\n",
       "       -5.24325675e-06, -4.06520036e-06, -2.38299882e-06, -2.32199015e-06,\n",
       "        1.16617505e-06, -6.28756970e-06, -3.77748347e-06, -1.35692066e-06,\n",
       "       -5.84517078e-06, -3.15820168e-06, -9.42642978e-07, -2.82503674e-06,\n",
       "       -3.78161514e-06, -9.20720951e-06, -5.89192842e-06, -1.50626249e-06,\n",
       "       -1.90224835e-06, -3.52442183e-06, -4.14778213e-06, -4.20650620e-06,\n",
       "       -6.62789194e-06, -6.59730898e-06, -1.79433312e-06,  7.04749128e-07,\n",
       "       -4.42027983e-06, -1.81368593e-06, -2.97886625e-06, -3.20856292e-06,\n",
       "        9.60467321e-08, -6.82393216e-06, -2.50283498e-06, -2.64603514e-06,\n",
       "        1.10112380e-06,  5.29948263e-07, -1.39659505e-06,  2.83866204e-07,\n",
       "        2.86350337e-06, -3.90047880e-06, -6.60342374e-06, -6.62992561e-06,\n",
       "       -6.28829882e-06, -3.96396433e-06])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 3685\n",
       "     nit: 2292\n",
       "    njev: 3685\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([ -1.28360069,  -1.95043776,  -3.50902598,   0.81429706,\n",
       "         5.9827344 ,  -3.04610152,   3.36758866,   1.21220259,\n",
       "         1.50258781,   4.30474057,  -0.78202422,   5.79946618,\n",
       "        -3.36319941,   9.80529823,   4.6837639 ,  -1.12246312,\n",
       "        -0.17111346,   6.76734263,   0.87889167,  10.04633224,\n",
       "         1.50578556,  -1.11318039,   9.69440423,   1.72990009,\n",
       "        -3.83384633,   2.03808807,   3.49014595,   0.81059209,\n",
       "         6.7919999 ,   1.86303386,   1.39393092,  -1.48121147,\n",
       "         2.69596557,   2.4326838 ,  -5.79396762, -10.21608112,\n",
       "        -7.88978101,  -1.76972362,  -1.19965331,  -6.99213099,\n",
       "        -5.57668802,  -5.12393528,  -2.71210481,  -4.88092421,\n",
       "        -9.91147008,  -1.72555543,   5.07965541,   1.94589059,\n",
       "         2.73649796,  -3.02181612])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [N,rho,sigma]\n",
    "res = fivefoldCV_q22(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neurons</th>\n",
       "      <th>rho</th>\n",
       "      <th>sigma</th>\n",
       "      <th>fun</th>\n",
       "      <th>init</th>\n",
       "      <th>err_tr</th>\n",
       "      <th>jac_norm</th>\n",
       "      <th>err_val</th>\n",
       "      <th>err_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.063565</td>\n",
       "      <td>2.470723</td>\n",
       "      <td>0.125164</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.185102</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neurons      rho  sigma       fun      init    err_tr  jac_norm   err_val  \\\n",
       "0     33.0  0.00001   1.11  0.063565  2.470723  0.125164  0.002681  0.185102   \n",
       "\n",
       "   err_test  \n",
       "0       NaN  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
